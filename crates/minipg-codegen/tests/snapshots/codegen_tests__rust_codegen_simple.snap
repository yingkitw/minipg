---
source: crates/minipg-codegen/tests/codegen_tests.rs
expression: code
---
// Generated parser for Calculator grammar
// DO NOT EDIT - This file is automatically generated

use std::fmt;

/// Parse error with context information.
#[derive(Debug, Clone, PartialEq)]
pub struct ParseError {
    pub message: String,
    pub position: usize,
    pub expected: Vec<String>,
    pub found: Option<String>,
}

impl ParseError {
    pub fn new(message: String, position: usize) -> Self {
        Self {
            message,
            position,
            expected: Vec::new(),
            found: None,
        }
    }

    pub fn with_expected(mut self, expected: Vec<String>) -> Self {
        self.expected = expected;
        self
    }

    pub fn with_found(mut self, found: String) -> Self {
        self.found = Some(found);
        self
    }
}

impl fmt::Display for ParseError {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        write!(f, "Parse error at position {}: {}", self.position, self.message)?;
        if !self.expected.is_empty() {
            write!(f, " (expected: {})", self.expected.join(", "))?;
        }
        if let Some(ref found) = self.found {
            write!(f, " (found: {})", found)?;
        }
        Ok(())
    }
}

impl std::error::Error for ParseError {}

/// Token with position information.
#[derive(Debug, Clone, PartialEq)]
pub struct Token {
    pub kind: TokenKind,
    pub text: String,
    pub position: usize,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum TokenKind {
    NUMBER,
    Eof,
}

/// AST node types.
#[derive(Debug, Clone)]
pub enum AstNode {
    Expr(Box<Expr>),
    Term(Box<Term>),
}

/// Listener trait for AST events.
pub trait Listener {
    fn enter_expr(&mut self, _node: &Expr) {}
    fn exit_expr(&mut self, _node: &Expr) {}
    fn enter_term(&mut self, _node: &Term) {}
    fn exit_term(&mut self, _node: &Term) {}
}

/// Lexer for Calculator grammar.
/// 
/// This lexer uses an optimized DFA (Deterministic Finite Automaton)
/// generated at compile time for efficient tokenization.
#[derive(Debug)]
pub struct CalculatorLexer {
    input: Vec<char>,
    position: usize,
}

impl CalculatorLexer {
    /// Create a new lexer from input string.
    #[inline]
    pub fn new(input: &str) -> Self {
        Self {
            input: input.chars().collect(),
            position: 0,
        }
    }

    /// Get the next token from the input.
    /// 
    /// Returns Ok(Token) on success, or Err(ParseError) if tokenization fails.
    pub fn next_token(&mut self) -> Result<Token, ParseError> {
        // Skip whitespace
        self.skip_whitespace();

        let start_pos = self.position;

        // Check for EOF
        if self.position >= self.input.len() {
            return Ok(Token {
                kind: TokenKind::Eof,
                text: String::new(),
                position: start_pos,
            });
        }

        // Use DFA for tokenization
        match self.next_token_dfa() {
            Some(mut token) => {
                token.position = start_pos;
                Ok(token)
            }
            None => {
                // Error recovery: skip invalid character and try again
                let invalid_char = self.input[self.position];
                self.position += 1;
                Err(ParseError::new(
                    format!("Unexpected character: '{}'", invalid_char),
                    start_pos,
                ).with_found(invalid_char.to_string()))
            }
        }
    }

    /// Tokenize all input and collect errors.
    /// 
    /// Returns all successfully parsed tokens and a list of errors encountered.
    pub fn tokenize_all(&mut self) -> (Vec<Token>, Vec<ParseError>) {
        let mut tokens = Vec::new();
        let mut errors = Vec::new();

        loop {
            match self.next_token() {
                Ok(token) => {
                    let is_eof = token.kind == TokenKind::Eof;
                    tokens.push(token);
                    if is_eof {
                        break;
                    }
                }
                Err(err) => {
                    errors.push(err);
                    // Continue tokenizing after error
                    if self.position >= self.input.len() {
                        break;
                    }
                }
            }
        }

        (tokens, errors)
    }

    #[inline(always)]
    fn skip_whitespace(&mut self) {
        while self.position < self.input.len() {
            match self.input[self.position] {
                ' ' | '\t' | '\r' | '\n' => self.position += 1,
                _ => break,
            }
        }
    }

    /// Character class lookup table.
    /// 
    /// Maps each character to its character class ID for efficient matching.
    /// This table is generated at compile time and stored as a const array.
    const CHAR_CLASS_TABLE: [u8; 256] = [
        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  // 0x00-0x0F
        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  // 0x10-0x1F
        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  // 0x20-0x2F
          0,   1,   2,   3,   4,   5,   6,   7,   8,   9, 255, 255, 255, 255, 255, 255,  // 0x30-0x3F
        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  // 0x40-0x4F
        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  // 0x50-0x5F
        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  // 0x60-0x6F
        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  // 0x70-0x7F
        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  // 0x80-0x8F
        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  // 0x90-0x9F
        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  // 0xA0-0xAF
        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  // 0xB0-0xBF
        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  // 0xC0-0xCF
        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  // 0xD0-0xDF
        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,  // 0xE0-0xEF
        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255 // 0xF0-0xFF

    ];

    /// Get the character class ID for a given character.
    /// 
    /// Returns 255 for characters not in any class.
    #[inline]
    fn get_char_class(ch: char) -> u8 {
        if (ch as u32) < 256 {
            Self::CHAR_CLASS_TABLE[ch as usize]
        } else {
            255 // Unknown class for non-ASCII
        }
    }

    /// Token type lookup table.
    /// 
    /// Maps token name strings to TokenKind for efficient conversion.
    #[inline]
    fn token_name_to_kind(name: &str) -> TokenKind {
        match name {
            "NUMBER" => TokenKind::NUMBER,
            _ => TokenKind::Eof,
        }
    }

    /// Match character using lookup table.
    /// 
    /// This is faster than multiple if/else or match statements.
    #[inline]
    fn match_char_fast(&self, ch: char, expected_class: u8) -> bool {
        Self::get_char_class(ch) == expected_class
    }

    /// Check if character is in a range using lookup table.
    #[inline]
    fn is_in_range(&self, ch: char, start_class: u8, end_class: u8) -> bool {
        let class = Self::get_char_class(ch);
        class >= start_class && class <= end_class
    }

    fn next_token_dfa(&mut self) -> Option<Token> {
        let mut state = 0;
        let mut token_start = self.position;
        let mut last_accepting: Option<(usize, &str)> = None;

        loop {
            // Check if current state is accepting
            match state {
                0 => last_accepting = Some((self.position, "NUMBER")),
                _ => {}
            }

            // Get next character
            let ch = match self.input.get(self.position) {
                Some(&c) => c,
                None => break,
            };

            // Transition to next state
            state = match (state, ch) {
                _ => break, // No valid transition
            };

            self.position += 1;
        }

        // Return token if we found an accepting state
        if let Some((end_pos, token_name)) = last_accepting {
            let text: String = self.input[token_start..end_pos].iter().collect();
            Some(Token {
                position: token_start,
                kind: match token_name {
                    "NUMBER" => TokenKind::NUMBER,
                    _ => TokenKind::Eof,
                },
                text,
            })
        } else {
            None
        }
    }
    // Lookup table stats: 10 chars, 10 classes, 256 bytes
}

/// Parser for Calculator grammar.
#[derive(Debug)]
pub struct CalculatorParser {
    tokens: Vec<Token>,
    position: usize,
}

impl CalculatorParser {
    #[inline]
    pub fn new(tokens: Vec<Token>) -> Self {
        Self { tokens, position: 0 }
    }

    /// Parse expr rule.
    pub fn parse_expr(&mut self) -> Result<AstNode> {
        // TODO: Implement rule parsing
        unimplemented!()
    }

    /// Parse term rule.
    pub fn parse_term(&mut self) -> Result<AstNode> {
        // TODO: Implement rule parsing
        unimplemented!()
    }

}
